http://www.cioage.com/art/201306/102545.htm

银行IT的那点事

导读：上周日，中国工商银行部门地区出现系统故障，包括柜台、取款机的业务都受到影响。此事发生在时间也很微妙，最近银行间市场资金紧张，一些市民怀疑工行的故障与“钱荒”有关，从而引发各界的解读和联想。.....

上周日，中国工商银行部门地区出现系统故障，包括柜台、取款机的业务都受到影响。此事发生在时间也很微妙，最近银行间市场资金紧张，一些市民怀疑工行的故障与“钱荒”有关，从而引发各界的解读和联想。一位银行IT工作者在社会化问答网站“知乎”上对此事做了解读，并介绍了银行IT背后的故事。

为什么银行IT会出问题?

1.现代IT系统非常复杂，当系统大到一定的程度，总会有失控的状况。世界上就从来都没有过没错误的复杂程序，问题只在于这个错误你有没有碰上而已。银行的系统是由很多不同软硬件厂商的产品拼在一起运作，复杂程度远超过普通家用电脑，这么简单的家用电脑还会死机呢....而且系统复杂到一定程度，就不是人多或者钱多就能完全解决问题的了。

2.要尽量不出问题，要钱，很多钱(比如中型银行建设一个过得去的容灾系统要上亿)。但出问题只是“有可能”，花的钱可是实实在在的。换了你是领导，你也不会无限制的向里面投钱。

3.稳定运行的最好的办法之一是不对系统进行改造。由于有新的业务要求，系统确实要不停的升级，每次变动对系统的稳定运行都是一个挑战。

银行IT为什么会大面积的出现问题?

因为三个字：大集中。最早之前，银行系统还没联网，一出问题只是某个区或者某个市。最近十多年银行业都在搞大集中:五大行除了中国银行之外的四家都已经完成了大集中。工行是第一家完成的，当年号称9991大集中工程，好像是1999年开始2002年完成。包括工农建交，国开，农发，浦发，华夏，民生等大部分的银行都是双中心运作，一个北京一个上海(交行好像有个中心在武汉，人行好像在无锡)。中国银行很早就集中成五大中心，至今还没弄成双中心。

大集中有很多业务上的好处，但从系统稳定性影响范围来说，就有点“所有鸡蛋都放在同一个篮子”里面的意味，虽然已经是好多好多好多人花好多好多钱去看好这个篮子了，但百密总有一疏，鸡蛋那么密都能孵得出小鸡呢!

为什么这种故障好像越来越多了?

以前没有微博没有微信，只要你不是倒霉的用户就不会知道出过问题。以前没有网银没有淘宝，你半夜不会买东西刷卡。好多年前我在某大行省行做升级，凌晨3点多的时候出了大问题，如果8点前搞不定就全省这银行就停业了，6点多的时候是行长站在后面看着我操作，最后7点多搞定。换成了今天压力估计更大了。

为什么淘宝，QQ，Google好像很少出故障?

因为四个字：历史原因。银行的IT建设从80年代开始，传统的思路还是集中在单台(有的多一台做成双机热备)服务器上跑程序。互联网的IT建设大部分都从21世纪开始，大多采用的是分布式的思路：由多台计算机同时在跑程序，其中一台出了问题影响也没那么大。

既然这么落后，为什么不转变?

银行程序的特点是要稳定，转变模式的风险很大(有的程序部分用的还是20年前的技术)。所以虽然也在慢慢的转，但起码到今天还没转多少。顺便感叹一下改革之难，赞颂一下邓伯伯。

为什么没有应急预案或者应急预案没有起作用?

银行IT是中国IT业中最严谨的行业。比如有的银行还要求厂商维护人员不能操作，只能银行员工操作。

大的变更一定会有预案，甚至换个硬盘，改个IP这种做过几百次的操作都会有预案。但预案与真实一般都有相当差距。上面已经提到系统非常复杂，可能出现的问题如果真全部写下来，可能有几百个分支。而且，系统的故障并不会根据你的应急预案来发生。

应急预案的最重要的作用是应付上级监管，根据应急预案搭好可能需要的应急软硬件环境，大致理清概要思路，以及锻炼团队。真有复杂问题，还是靠牛人现场解决的多。

假如银行系统出问题的时候，我的存款会不会多了或者少了?

常见的最简单的衡量连续运行系统的整体指标有RTO和RPO，不严谨的说大致就是停业多久和数据丢多少的指标。

大家可以放心存钱在银行。一般出现问题也只是在停业(某个时间的系统不能运行)这个层面，还没到丢数据或者数据错的层面。就算真出了丢数据的问题，准确的数据一般可以从备份中心或者容灾中心里面捞回来。银行系统每天晚上都要对账，会保证数据准确。

不过为了防止系统故障，多办几个不同银行的卡倒是有必要的，我自己的三张卡除了分布在不同银行外，还横跨Visa/Master/AE，总有一张能刷到啊....

为什么要停几个小时这么久?

先说定位问题的时间：从发现问题上报到IT信息中心(或者在监控系统发现问题)，IT中心的人开始查系统，定位故障原因，如果定位不清还要找相关的软硬件人员到场或者远程网络支持(基于安全原因，银行大部分都不能远程网络查看系统，维护人员到数据中心也需要时间，如果还堵车.....)，找出问题的根源，一小时算超快的了。类似你莫名高烧，到底是哪个器官出问题，去医院做检查做判断总需要时间吧?

解决问题就更不好说了，其实和大家的电脑一样，往往重启是最有效的方法，但很多业务系统部分出现问题是不能重启的(可能会影响别的业务系统)。至今国外各大厂商的标准维护合同，绝大部分都没有承诺修复时间。

根据手头的一份略过时的银监会突发事件应急管理规范：一个省停业6个小时以上才算I级特别重大突发事件，3小时是II级，半小时以上是III级。以管窥豹，落叶知秋，几小时真不算什么。

不是说有容灾和备份吗?为啥不快速切过去就好了?

这是一个很常见的误解：出了故障的时候，有备份系统和容灾系统就可以很快恢复业务。现实是这样吗?

先说备份系统，常规备份系统是不能运行业务程序的：备份一般只是把数据保存多一份或者几份，一般是在丢数据的时候才用来恢复，而且恢复的时间很多都在几小时以上。类似大家手头只有一个avi文件，没有播放软件也没法看啊，只不过银行的“播放软件”要架设起来就复杂了.....

再说容灾系统，强调一个连很多IT人都不清楚的事实：银行容灾系统不会轻易启用整体切换!前面已经说了，IT系统已经这么复杂了，容灾系统相当于再复制一套，复杂性增加了不止2倍。切换起来是非常麻烦，非常伤筋动骨，惊动非常多人力物力，不是碰到大灾大难(比如地震，机房着火，恐怖分子爆炸之类)不会进行切换。

当然平时会进行容灾切换演练，但一般不会拿核心系统来真实切换，原因是有风险。以前也出现过华东某省级行切换到了容灾中心后切不回生产中心的悲催惨剧。最近西北某地农信社成功的把核心生产切到了容灾系统上，比较不简单，不过这毕竟是独立法人的小银行，大行不是这么个玩法。

另外，看到有不少评论说“没人敢担风险切换到灾备节点上”。其实一般是这样的：建好容灾系统之后往往都会写一套DRP(灾难恢复计划)或者BCP(业务连续性计划)，就是容灾系统启动的流程方案，里面会规定好什么场景下由什么人拍板切换到灾备中心，一般不会真出问题才临时来拍脑袋来想，(当然临时调整也是有可能的)，也不是谁说切换就谁去背黑锅。
